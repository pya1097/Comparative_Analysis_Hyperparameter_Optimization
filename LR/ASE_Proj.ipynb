{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "el_-zlazLRUM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\athud\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
        "\n",
        "# max_iter=100,50,500,1000,5000\n",
        "# C=1,0.05,0.5,5,10,100\n",
        "# tol=0.0001,0.001,0.003,0.005,0.0003,0.0005,0.0008\n",
        "# fit_intercept=True, False\n",
        "# penalty='l2',None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJqPYDdwLSY1",
        "outputId": "b0daa204-4c03-4a2d-f07d-6d0b736692f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.969444\n"
          ]
        }
      ],
      "source": [
        "#1 digits\n",
        "X, y = datasets.load_digits(return_X_y=True)\n",
        "X = X / X.max()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuhtoKf4NLCa",
        "outputId": "ed690728-96ad-4514-ee08-21470bd16a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.912281\n"
          ]
        }
      ],
      "source": [
        "#2 breast cancer\n",
        "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
        "X = X / X.max()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8HkS68QNkTZ",
        "outputId": "59bce718-1071-40b5-e566-c63946afe89e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.933333\n"
          ]
        }
      ],
      "source": [
        "#3 iris\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X = X / X.max()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yISaKCxbNtUR",
        "outputId": "0e34d66b-5a4c-4d60-bf8f-e1d3b05c38c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.722222\n"
          ]
        }
      ],
      "source": [
        "#4 wine\n",
        "X, y = datasets.load_wine(return_X_y=True)\n",
        "X = X / X.max()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "R_iDzzzswlZd"
      },
      "outputs": [],
      "source": [
        "#5 social network ads\n",
        "df=pd.read_csv('Social_Network_Ads.csv')\n",
        "data = df.drop(columns=['User ID'])\n",
        "data = pd.get_dummies(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLXSPB4vr4H3",
        "outputId": "603ca7a1-9737-440e-858e-a0f633eca037"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.625000\n"
          ]
        }
      ],
      "source": [
        "predictions = ['Age', 'EstimatedSalary','Gender_Female', 'Gender_Male']\n",
        "X = data[predictions]\n",
        "y = data['Purchased']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "HvkJhhpp2ElU"
      },
      "outputs": [],
      "source": [
        "#6 heart disease\n",
        "df=pd.read_csv('heart_disease.csv')\n",
        "\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "cols_to_scale=['age','education','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
        "scaler=MinMaxScaler()\n",
        "df[cols_to_scale]=scaler.fit_transform(df[cols_to_scale])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ejCepjc4bNG",
        "outputId": "bb8f7a0d-a27d-4fa5-de8f-c77fc469833a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.857923\n"
          ]
        }
      ],
      "source": [
        "X=df.drop('TenYearCHD',axis='columns')\n",
        "y=df.TenYearCHD\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "avBrbGer_p58"
      },
      "outputs": [],
      "source": [
        "#7 Titanic\n",
        "df=pd.read_csv('titanic2.csv')\n",
        "df = df[[\"Age\", \"Fare\", \"Sex\", \"sibsp\", \"Parch\", \"Pclass\", \"Embarked\", \"Survived\"]]\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8s2hflgXAH8D",
        "outputId": "f9cd981e-a6aa-417e-c2c2-1daa32389d24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.770992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\athud\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
            "c:\\Users\\athud\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
          ]
        }
      ],
      "source": [
        "X = df.drop(['Survived',], axis=1)\n",
        "y = df['Survived']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "j8qfr3k5Cf8i"
      },
      "outputs": [],
      "source": [
        "#8 Employee attrition\n",
        "df=pd.read_csv('employee_attr.csv')\n",
        "df.drop('EmployeeID', axis=1, inplace=True)\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Attrition'] = le.fit_transform(df['Attrition'])\n",
        "df['Gender'] = le.fit_transform(df['Gender'])\n",
        "df['Over18'] = le.fit_transform(df['Over18'])\n",
        "df = pd.get_dummies(df, columns=['BusinessTravel', 'Department', 'EducationField', 'JobRole', 'MaritalStatus'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvoYJV8mEWwu",
        "outputId": "4961e431-0501-4c17-a895-7d0308fef5b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.826682\n"
          ]
        }
      ],
      "source": [
        "X = df.drop('Attrition', axis=1)\n",
        "y = df['Attrition']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "SKTMWvCjFqCG"
      },
      "outputs": [],
      "source": [
        "#9 Pumpkin seeds\n",
        "df=pd.read_csv('pumpkin_seeds.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9-Yq9ZiG_FK",
        "outputId": "b1563f8f-4e50-4d51-d520-4d2d079ee4d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.862000\n"
          ]
        }
      ],
      "source": [
        "X = df.drop(columns='Class').values\n",
        "y = df['Class']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "B3xeiJDaHBpW"
      },
      "outputs": [],
      "source": [
        "#10 Marketing campaign\n",
        "df=pd.read_csv('Marketing_campaigns.csv')\n",
        "location=pd.get_dummies(df[\"Location\"],drop_first=True)\n",
        "df.drop(\"Location\",axis=1,inplace=True)\n",
        "df=pd.concat([df,location],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-uEeEjmJUlK",
        "outputId": "12c0de6a-2246-4e94-a8ac-7770723c54ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.250000\n"
          ]
        }
      ],
      "source": [
        "X=df.drop(\"Email Opened\",axis=1)\n",
        "y=df[\"Email Opened\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5Ay90N13JXzR"
      },
      "outputs": [],
      "source": [
        "#11 bank loan\n",
        "df=pd.read_csv('bankloan.csv')\n",
        "standrd_scaler=StandardScaler()\n",
        "columns=['Age','Experience','Income','Family','CCAvg','Mortgage','Education','ZIP.Code']\n",
        "for col in columns:\n",
        "    df[col]=standrd_scaler.fit_transform(np.array(df[col]).reshape(-1,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oROyyxl0Q6PG",
        "outputId": "859f9b91-c9a7-4652-fc9e-7c9b9d6b33a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.945000\n"
          ]
        }
      ],
      "source": [
        "X=df.drop(['ID','Personal.Loan'],axis=1)\n",
        "y=df['Personal.Loan']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4B81CGR1RpPa"
      },
      "outputs": [],
      "source": [
        "#12 date fruit\n",
        "df=pd.read_csv('datefruit.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCscKGHRUI4T",
        "outputId": "6d05f037-b733-4dce-d2da-516fd1f934b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.538889\n"
          ]
        }
      ],
      "source": [
        "X = df.drop([\"Class\"], axis = 1)\n",
        "y = df[\"Class\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "NY_99ppxUm-r"
      },
      "outputs": [],
      "source": [
        "#13 fake bills\n",
        "df=pd.read_csv('fake_bills.csv')\n",
        "df.dropna(inplace=True)\n",
        "df.is_genuine=[1 if i == True else 0 for i in df.is_genuine]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQk1zN9gYAlH",
        "outputId": "d089a869-98aa-432a-9929-cb5ffa904e14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.996587\n"
          ]
        }
      ],
      "source": [
        "y=df.is_genuine.values\n",
        "X=df.drop([\"is_genuine\"],axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Jw0FnITKZfXX"
      },
      "outputs": [],
      "source": [
        "#14 Employee turnover\n",
        "df=pd.read_csv('turnover.csv', encoding = 'ISO-8859-1')\n",
        "df.dropna(inplace=True)\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        values = df[col].value_counts()\n",
        "        values = dict(values)\n",
        "        label = LabelEncoder()\n",
        "        label = label.fit(df[col])\n",
        "        df[col] = label.transform(df[col].astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ7MXdWGbenM",
        "outputId": "19f9f402-6a8c-42ec-b286-a8dcd7379237"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.535398\n"
          ]
        }
      ],
      "source": [
        "X = df.drop(columns=['event'])\n",
        "y = df['event']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PjnAwcx_cYQY"
      },
      "outputs": [],
      "source": [
        "#15 Cancer\n",
        "df=pd.read_csv('Cancer_Data.csv')\n",
        "df.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace = True)\n",
        "df.diagnosis = [1 if each == \"M\" else 0 for each in df.diagnosis]\n",
        "x = df.drop([\"diagnosis\"],axis = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hs6JDLwdz58",
        "outputId": "883d4474-9097-45d4-baca-b6db73446839"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.912281\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\athud\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
            "c:\\Users\\athud\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
            "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
          ]
        }
      ],
      "source": [
        "X = ((x - np.min(x))/(np.max(x)-np.min(x)))\n",
        "y = df.diagnosis\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "ulgBsByMeIKW"
      },
      "outputs": [],
      "source": [
        "#16 wine\n",
        "df=pd.read_csv('wine.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eoEKccwg0We",
        "outputId": "6b7949ab-284e-4b32-ebbb-f986235dbbbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.606250\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "X = df.drop('quality',axis = 1)\n",
        "y = df['quality']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cS0WAt53j5eq"
      },
      "outputs": [],
      "source": [
        "#17 kidney stone\n",
        "df=pd.read_csv('kidney_stone_data.csv')\n",
        "le=LabelEncoder()\n",
        "df['stone_size']=le.fit_transform(df['stone_size'])\n",
        "df['treatment']=le.fit_transform(df['treatment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh9dXIlLlJBO",
        "outputId": "653a87e2-ee95-4d17-dab6-47ddc8da7ea2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.800000\n"
          ]
        }
      ],
      "source": [
        "X=df.iloc[:,0:2]\n",
        "y=df.iloc[:,2]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "m07XDhzZlcv2"
      },
      "outputs": [],
      "source": [
        "#18 rock or mine\n",
        "df=pd.read_csv('rock_mine.csv',header = None)\n",
        "df.groupby(60).mean()\n",
        "df[60] = [1 if each == \"M\" else 0 for each in df[60]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a03cEZh5m_sr",
        "outputId": "22939f1a-ee67-4237-ec75-24107fe4b73d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.809524\n"
          ]
        }
      ],
      "source": [
        "X = df.drop(columns=60, axis=1)\n",
        "y = df[60]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3lJCf6MWnmXW"
      },
      "outputs": [],
      "source": [
        "#19 gender voice\n",
        "df=pd.read_csv('gender_voice.csv')\n",
        "df.dropna(inplace=True)\n",
        "df['label'] = [1 if each == \"male\" else 0 for each in df['label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_JB9tC6hoZsz",
        "outputId": "354473c0-583d-497d-fa21-19a71b2e2d42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.477918\n"
          ]
        }
      ],
      "source": [
        "y = df['label']\n",
        "x = df.drop([\"label\"],axis=1)\n",
        "X = ((x-np.min(x))/(np.max(x)-np.min(x))).values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Yt8ugSGhoihb"
      },
      "outputs": [],
      "source": [
        "#20 possum\n",
        "df=pd.read_csv('possum.csv')\n",
        "df.drop(['case', 'site','Pop'], axis=1, inplace=True)\n",
        "df['sex'] = [1 if each == \"m\" else 0 for each in df['sex']]\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgghcpD-t95V",
        "outputId": "7c788c54-8c34-412d-8518-ea47881f30c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LogisticRegression score: 0.523810\n"
          ]
        }
      ],
      "source": [
        "y = df.sex.copy()\n",
        "X = df.drop(['sex'], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression(max_iter=1000, C=1, tol=0.0001, fit_intercept=True, penalty='l2')\n",
        "print(\"LogisticRegression score: %f\" % logistic.fit(X_train, y_train).score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsoYH9_-eB6q"
      },
      "source": [
        "# OPTUNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGgOOI3peS9i",
        "outputId": "97fee878-ea99-4b9c-bb45-dd3ee5c0442e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.4/233.4 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (24.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.29)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.66.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Installing collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.3 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "S6S-oREjuSWG"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZSVr_TABeQVM"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  y = df.sex.copy()\n",
        "  X = df.drop(['sex'], axis=1)\n",
        "\n",
        "  params = {\n",
        "      'tol' : trial.suggest_uniform('tol' , 1e-6 , 1e-3),\n",
        "      'C' : trial.suggest_loguniform(\"C\", 1e-2, 1),\n",
        "      'max_iter' : trial.suggest_int('max_iter', 100, 5000, step=100),\n",
        "      'fit_intercept' : trial.suggest_categorical('fit_intercept', [True, False]),\n",
        "      'penalty' : trial.suggest_categorical('penalty', ['l2', None])\n",
        "  }\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "  logistic = linear_model.LogisticRegression(**params)\n",
        "  accuracy = logistic.fit(X_train, y_train).score(X_test, y_test)\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXaBbjNfohs",
        "outputId": "8c489950-0807-4357-f44e-4d0d00f5275d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "numbers of the finished trials: 500\n",
            "the best params: {'tol': 0.000871546752047836, 'C': 0.08727816246861037, 'max_iter': 1800, 'fit_intercept': True, 'penalty': 'l2'}\n",
            "the best value: 0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "study = optuna.create_study(direction = 'maximize' , study_name = 'lr' , pruner = optuna.pruners.HyperbandPruner())\n",
        "study.optimize(objective, n_trials = 500)\n",
        "print('numbers of the finished trials:' , len(study.trials))\n",
        "print('the best params:' , study.best_trial.params)\n",
        "print('the best value:' , study.best_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZkN6RRThqNH"
      },
      "source": [
        "# HYPEROPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "noDRsfBTgeXz"
      },
      "outputs": [],
      "source": [
        "from hyperopt import hp, tpe, fmin, Trials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L84zUmYQj2av"
      },
      "outputs": [],
      "source": [
        "def objective(params):\n",
        "  params['max_iter'] = int(params['max_iter'])\n",
        "  y = df.sex.copy()\n",
        "  X = df.drop(['sex'], axis=1)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "  logistic = linear_model.LogisticRegression(**params)\n",
        "  accuracy = logistic.fit(X_train, y_train).score(X_test, y_test)\n",
        "\n",
        "  return -accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9e3re8Qjc_0",
        "outputId": "8b3ccb5c-4d05-468b-cbdc-95fb09653f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:26<00:00, 19.10trial/s, best loss: -0.5238095238095238]\n",
            "Best hyperparameters: {'C': 2.4467971142823086, 'fit_intercept': 0, 'max_iter': 3300.0, 'penalty': 0, 'tol': 0.0007238365358722}\n",
            "Best objective value: 0.5238095238095238\n"
          ]
        }
      ],
      "source": [
        "params = {\n",
        "      'tol' : hp.uniform('tol' , 1e-6 , 1e-3),\n",
        "      'C' : hp.loguniform(\"C\", 1e-2, 1),\n",
        "      'max_iter' : hp.quniform('max_iter', 100, 5000, 100),\n",
        "      'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
        "      'penalty' : hp.choice('penalty', ['l2', None])\n",
        "  }\n",
        "tpe_algo = tpe.suggest\n",
        "trials = Trials()\n",
        "\n",
        "# Run hyperparameter optimization\n",
        "best_params = fmin(\n",
        "    fn=objective,  # Objective function to minimize\n",
        "    space=params,  # Search space\n",
        "    algo=tpe_algo,  # Optimization algorithm\n",
        "    max_evals=500,  # Maximum number of evaluations\n",
        "    trials=trials,  # Trials object for tracking progress\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "best_loss = min(trials.losses())\n",
        "print(\"Best objective value:\", -best_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc6w5NrJ1ox2"
      },
      "source": [
        "# SKOPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ljisSqb-JYb"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-optimize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9B0TdStN-Hse"
      },
      "outputs": [],
      "source": [
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "sA0rxm724Gc-"
      },
      "outputs": [],
      "source": [
        "y = df.sex.copy()\n",
        "X = df.drop(['sex'], axis=1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=23)\n",
        "logistic = linear_model.LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu4GF4VC-Aak",
        "outputId": "b8338152-6cb7-444d-81b7-ca09426cd9e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
            "Best hyperparameters found:  OrderedDict([('C', 0.389488143609897), ('fit_intercept', False), ('max_iter', 5000), ('penalty', 'l2'), ('tol', 0.009282429135060194)])\n",
            "Best accuracy score found:  0.6633428300094967\n"
          ]
        }
      ],
      "source": [
        "param_space = {\n",
        "    'C': Real(1e-6, 1e+6, prior='log-uniform'),\n",
        "    'penalty': Categorical([None, 'l2']),\n",
        "    'max_iter': Integer(50, 5000),\n",
        "    'tol': Real(1e-6, 1e-2, prior='uniform'),\n",
        "    'fit_intercept': Categorical([True, False])\n",
        "}\n",
        "\n",
        "opt = BayesSearchCV(\n",
        "    logistic,\n",
        "    param_space,\n",
        "    scoring='accuracy',\n",
        "    n_iter=100,\n",
        "    cv=3,\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "opt.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best hyperparameters found: \", opt.best_params_)\n",
        "print(\"Best accuracy score found: \", opt.best_score_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftMBLGjd-D8I"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
